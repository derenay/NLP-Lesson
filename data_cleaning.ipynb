{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Cleaning Tasks:\n",
    "You need to apply the following modern NLP preprocessing techniques:\n",
    "\n",
    "1Ô∏è‚É£ Lowercasing: Convert the text to lowercase while keeping important words standardized (e.g., \"NLP\" should remain unchanged).\n",
    "2Ô∏è‚É£ Removing Special Characters & Emojis: Strip out emojis (ü§ñüìöüöÄ) and unnecessary punctuation (!, ..., etc.).\n",
    "3Ô∏è‚É£ Removing Stopwords: Eliminate common words that do not contribute much meaning (e.g., \"and\", \"but\", \"also\").\n",
    "4Ô∏è‚É£ Lemmatization: Reduce words to their base forms (e.g., \"learning\" ‚Üí \"learn\").\n",
    "5Ô∏è‚É£ Spelling Normalization: Ensure all variations of NLP are standardized to \"NLP\".\n",
    "\n",
    "# üìå Recommended Tools & Techniques:\n",
    "You should use modern NLP libraries to clean the text efficiently:\n",
    "\n",
    "spaCy ‚Üí For tokenization, stopword removal, and lemmatization.\n",
    "Hugging Face Tokenizer ‚Üí For advanced token processing.\n",
    "Regular Expressions (RegEx) ‚Üí For removing emojis and special characters.\n",
    "Custom Normalization Rules ‚Üí To standardize words like \"Nlp\" to \"NLP\".\n",
    "üìå Your Task:\n",
    "1Ô∏è‚É£ Implement the cleaning steps and return the final cleaned text.\n",
    "2Ô∏è‚É£ If you get stuck, tell me where you need help, and I‚Äôll guide you.\n",
    "\n",
    "üöÄ Let‚Äôs see your cleaned version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_data = \"Hello!!! I'm learning Natural Language Processing (NLP)...ü§ñ This field üìö is evolving very fast. However, some words (and, with, also, but) might be unnecessary! Also, different spelling variations exist; for example, NLP can be written as NLP, Nlp, or nlp. We need to normalize this as well. üöÄ\"\n",
    "# Load spaCy's English model (optimized for speed)\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Hugging Face Tokenizer (can handle text normalization efficiently)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello im learning natural language processing nlp this field  is evolving very fast however some words and with also but might be unnecessary also different spelling variations exist for example nlp can be written as nlp nlp or nlp we need to normalize this as well \n"
     ]
    }
   ],
   "source": [
    "new_data = re.sub(r'[^\\w\\s]', '', uncleaned_data.lower())\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'm', 'learn', 'natural', 'language', 'processing', 'nlp', 'field', ' ', 'evolve', 'fast', 'word', 'unnecessary', 'different', 'spelling', 'variation', 'exist', 'example', 'nlp', 'write', 'nlp', 'nlp', 'nlp', 'need', 'normalize']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(new_data)\n",
    "filtered_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_tokens = ['NLP' if token.lower() in ['nlp', 'nlp','nlp'] else token for token in filtered_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_tokens = tokenizer.convert_tokens_to_string(normalize_tokens)\n",
    "print(encode_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
