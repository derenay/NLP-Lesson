{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           text     label\n",
      "0            I love this product, it's amazing!  Positive\n",
      "1               Absolutely terrible, I hate it.  Negative\n",
      "2       The food was okay, but nothing special.   Neutral\n",
      "3     This is the worst movie I have ever seen!  Negative\n",
      "4  Great customer service, very friendly staff.  Positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Erenay\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"punkt\")  # Ensure tokenization works\n",
    "\n",
    "# Manually labeled dataset (training data)\n",
    "labeled_data = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I love this product, it's amazing!\",\n",
    "        \"Absolutely terrible, I hate it.\",\n",
    "        \"The food was okay, but nothing special.\",\n",
    "        \"This is the worst movie I have ever seen!\",\n",
    "        \"Great customer service, very friendly staff.\"\n",
    "    ],\n",
    "    \"label\": [\"Positive\", \"Negative\", \"Neutral\", \"Negative\", \"Positive\"]\n",
    "})\n",
    "print(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00713846  0.0012435  -0.00717887 -0.00224578  0.00371868  0.00583313\n",
      "  0.00119666  0.00210257 -0.00411176  0.00722634 -0.00630901  0.00465065\n",
      " -0.00822265  0.00203721 -0.00497833 -0.00424626 -0.00310521  0.00565903\n",
      "  0.00579819 -0.00497913  0.00077368 -0.00849669  0.00781208  0.00925818\n",
      " -0.00274128  0.00080249  0.00074291  0.00547578 -0.00860662  0.0005866\n",
      "  0.00687291  0.00223417  0.00112475 -0.00932484  0.00848336 -0.00626395\n",
      " -0.00299447  0.00349414 -0.00077323  0.00140776  0.00178264 -0.00683217\n",
      " -0.00972261  0.00904076  0.00619794 -0.00691516  0.00340173  0.00020311\n",
      "  0.00475436 -0.00712241  0.00402688  0.00434602  0.00995426 -0.00447361\n",
      " -0.00139072 -0.00731981 -0.00969978 -0.00908355 -0.00102616 -0.0065041\n",
      "  0.00485221 -0.00616474  0.00252195  0.00074225 -0.00339018 -0.00097752\n",
      "  0.00998255  0.00914521 -0.00446395  0.00908185 -0.0056444   0.00593039\n",
      " -0.00310098  0.00343292  0.00302053  0.00690109 -0.00237537  0.00877899\n",
      "  0.00759061 -0.00954758 -0.0080064  -0.0076394   0.00292382 -0.00279296\n",
      " -0.00693091 -0.00812905  0.00831401  0.00199299 -0.00933085 -0.00479404\n",
      "  0.00313711 -0.00471274  0.00528262 -0.00423492  0.00264066 -0.00804896\n",
      "  0.00621176  0.00481953  0.00078481  0.00301741]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences  = [word_tokenize(re.sub(r'[^\\w\\s]', '', text.lower())) for text in labeled_data['text']]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "word2vec_model.save(\"word2vec_labeling.model\")\n",
    "print(word2vec_model.wv[\"love\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_vec(sentence, model):\n",
    "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', sentence.lower()))\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if len(word_vectors) == 0:\n",
    "        return np.zeros(model.vector_size)  # Return zero vector if no words are in vocabulary\n",
    "    \n",
    "    return np.mean(word_vectors, axis=0)  # Average word vectors to get sentence vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 100)\n"
     ]
    }
   ],
   "source": [
    "# Convert labeled data to vector format\n",
    "X_train = np.array([sentence_to_vec(text, word2vec_model) for text in labeled_data[\"text\"]])\n",
    "y_train = labeled_data[\"label\"]\n",
    "\n",
    "print(X_train.shape)  # Check vector dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weak model trained successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train logistic regression model on Word2Vec embeddings\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Weak model trained successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text predicted_label\n",
      "0  The product is good but expensive and so terrible        Negative\n",
      "1       Terrible experience, I will never come back!        Negative\n",
      "2               It was fine, nothing too impressive.        Negative\n",
      "3            Best purchase I've made in a long time.        Positive\n",
      "4                      Not bad, but could be better.        Positive\n"
     ]
    }
   ],
   "source": [
    "# Unlabeled dataset\n",
    "unlabeled_data = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"The product is good but expensive and so terrible\",\n",
    "        \"Terrible experience, I will never come back!\",\n",
    "        \"It was fine, nothing too impressive.\",\n",
    "        \"Best purchase I've made in a long time.\",\n",
    "        \"Not bad, but could be better.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "# Convert unlabeled data into Word2Vec features\n",
    "X_unlabeled = np.array([sentence_to_vec(text, word2vec_model) for text in unlabeled_data[\"text\"]])\n",
    "\n",
    "# Predict labels using the trained model\n",
    "unlabeled_data[\"predicted_label\"] = model.predict(X_unlabeled)\n",
    "\n",
    "print(unlabeled_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
